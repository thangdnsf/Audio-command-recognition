#%% md

# Scientific Computing - Practice sessions  : Audio command recognition by DTW and classification

**Group name:**

**Names :**

**Surnames :**

The Practice sessions will permit testing the dynamic programming algorithm (DTW) seen in Exercise session (TD) and then implement an audio recognition system for isolated words (constituting orders for drones).


These sessions are divided into 3 parts:
- Part I: DTW and application of the TD
- Part II: Audio control word recognition system
- Part III: Comparison of dynamic programming with a classification method after data pre-processing by PCA

For **parts II and III**, you will test the audio recognition system on two sets of voices that will serve as a learning base (references) and a test base (sounds to be recognized) respectively. The list of the 13 drone commands are: *Landing, Takeoff, Takeoff, Advance, Right turn, Backward, Left turn, Right, Flip, Left, Stop, Higher, Lower and State of Emergency.*

To do this, you must per group of 2 students (number of students **MANDATORY**):
1. **Propose a study** that you will detail on a report.
For example, *influence male voices VS female voices, compare your own voices to the database, test the impact of different background noises on recognition...*];
2. Create, according to the objective of your study, your own learning base and test base from the proposed corpus and the voices and sounds you have recorded   [*audio parameters: 16 KHz, mono, 16 bits, *.wav format**];
3. Test the DTW and a classification method with pre-processing by PCA;
4. Evaluate the results;
5. Write a pdf report presenting the study, the results by the 2 methods and your comments and conclusions on your study (Max. length: 8 pages).



#%%

import matplotlib.pyplot as plt
import numpy
import scipy
import sklearn
import math
import os
import noisereduce as nr
import scikitplot as skplot
import librosa

#%% md

# Part I: Implementation of the dynamic programming algorithm

#%% md

1. Write a function in DTW python that implements the calculation and display of the cost matrix defined in TD.

2. In order to easily adapt the cost calculation according to the nature of the data (and therefore the distances used), write a function for each distance (Euclidean, letters, sounds) that will appear as a parameter of the DTW function.

#%%

def distance_euclidean(_lhs, _rhs):
    if _lhs > _rhs:
        return _lhs - _rhs
    else:
        return _rhs - _lhs

def distance_dna(_lhs, _rhs):
    if _lhs == _rhs:
        return 0
    else:
        return 1

def distance_sound(_lhs, _rhs):
    if _lhs == _rhs:
        return 0

    if _lhs == 'U' or _rhs == 'U':
        return 1

    if _lhs == 'V' or _rhs == 'V':
        return 2

    return 1

def dtw(_x: numpy.array, _y: numpy.array, _omegas: numpy.array, _distance, _skip=numpy.inf):
    assert len(_x) > 0
    assert len(_y) > 0
    assert len(_omegas) == 3

    if len(_x) < len(_y):
        _x, _y = _y, _x

    _offset = len(_x) - len(_y)

    _matrix = numpy.full((len(_y), len(_x)), numpy.inf)

    # top left corner is always bound to the diagonal frame
    _matrix[0, 0] = _omegas[1] * _distance(_x[0], _y[0])

    # first row elements are always bound to the left frame
    for _i in range(1, min(len(_x), 1 + _offset + _skip)):
        _matrix[0, _i] = _matrix[0, _i - 1] + _omegas[0] * _distance(_x[_i], _y[0])

    # first column elements are always bound to the top frame
    for _j in range(1, min(len(_y), 1 + _skip)):
        _matrix[_j, 0] = _matrix[_j - 1, 0] + _omegas[2] * _distance(_x[0], _y[_j])

    # rest of the matrix
    for _j in range(1, len(_y)):
        for _i in range(max(1, _j - _skip), min(len(_x), 1 + _j + _offset + _skip)):
            _d = _distance(_x[_i], _y[_j])
            _matrix[_j, _i] = min(
                _matrix[_j, _i - 1] + _omegas[0] * _d,
                _matrix[_j - 1, _i - 1] + _omegas[1] * _d,
                _matrix[_j - 1, _i] + _omegas[2] * _d
            )

    return _matrix


def dtw_d(_x: numpy.array, _y: numpy.array, _matrix: numpy.array):
    assert len(_x) > 0
    assert len(_y) > 0
    assert _matrix.shape == (len(_x), len(_y)) or _matrix.shape == (len(_y), len(_x))

    if len(_x) < len(_y):
        _x, _y = _y, _x

    return _matrix[len(_y) - 1, len(_x) - 1] / (len(_x) + len(_y))

def dtw_path(_x: numpy.array, _y: numpy.array, _matrix: numpy.array):
    assert len(_x) > 0
    assert len(_y) > 0
    assert _matrix.shape == (len(_x), len(_y)) or _matrix.shape == (len(_y), len(_x))

    if len(_x) < len(_y):
        _x, _y = _y, _x

    _output = []

    _i = len(_x) - 1
    _j = len(_y) - 1

    _output.append((_i, _j))

    while _i != 0 and _j != 0:
        _min = min(_matrix[_j, _i - 1], _matrix[_j - 1, _i - 1], _matrix[_j - 1, _i])
        if _matrix[_j - 1, _i - 1] == _min:
            _i -= 1
            _j -= 1
        elif _matrix[_j, _i - 1] == _min:
            _i -= 1
        else:
            _j -= 1

        _output.append((_i, _j))

    while _i != 0:
        _i -= 1
        _output.append((_i, _j))

    while _j != 0:
        _j -= 1
        _output.append((_i, _j))

    return _output

#%% md

#### Application to exercises

1. Test your programs on the exercises seen in TD.

2. Modify the local constraints, i.e. the weights according to the directions.

3. Add the consideration of global constraints, i. e. non-calculation when the boxes are too far from the diagonal (see exercise TD DNA sequence). From which position do global constraints not change the results?

#%%

def show_matrix(_title: str, _x: numpy.array, _y: numpy.array, _matrix: numpy.array, _path: numpy.array):
    _figure, _axes = plt.subplots()
    _axes.imshow(_matrix)

    _axes.xaxis.tick_top()

    plt.xticks(numpy.arange(len(_x)), _x)
    plt.yticks(numpy.arange(len(_y)), _y)

    _axes.set_ylim(len(_y)-0.5, -0.5)

    for _i in range(len(_x)):
        for _j in range(len(_y)):
            _axes.text(_i, _j, _matrix[_j, _i], ha="center", va="center", color="black")

    _axes.set_title(_title)
    _figure.tight_layout()

    plt.plot([_x[0] for _x in _path], [_x[1] for _x in _path], 'w')

    plt.show()

def resolve_td(title, _x: numpy.array, _y: numpy.array, _o: numpy.array, _distance, skip=numpy.inf):
    _matrix = dtw(_x, _y, _o, _distance, skip)
    _path = dtw_path(_x, _y, _matrix)
    _d = dtw_d(_x, _y, _matrix)

    show_matrix(title + ": d = " + str(_d), _x, _y, _matrix, _path)

resolve_td("TD 2.1", numpy.array([-2, 10, -10, 15, -13, 20, -5, 14, 2]), numpy.array([3, -13, 14, -7, 9, -2]), numpy.array([1, 1, 1]), distance_euclidean)

resolve_td("TD 2.2", numpy.array(['A', 'T', 'G', 'G', 'T', 'A', 'C', 'G', 'T', 'C']), numpy.array(['A', 'A', 'G', 'T', 'A', 'G', 'G', 'C']), numpy.array([1, 1, 1]), distance_dna)

dtw_2_3_cinq = numpy.array(['X', 'V', 'V', 'C', 'X'])
dtw_2_3_vingt = numpy.array(['UX', 'V', 'V'])
dtw_2_3_cent = numpy.array(['X', 'V', 'V'])

resolve_td("TD 2.3.1.cinq", numpy.array(['X', 'X', 'V', 'U', 'X', 'C', 'X']), dtw_2_3_cinq, numpy.array([1, 2, 1]), distance_sound)
resolve_td("TD 2.3.1.vingt", numpy.array(['X', 'X', 'V', 'U', 'X', 'C', 'X']), dtw_2_3_vingt, numpy.array([1, 2, 1]), distance_sound)
resolve_td("TD 2.3.1.cent", numpy.array(['X', 'X', 'V', 'U', 'X', 'C', 'X']), dtw_2_3_cent, numpy.array([1, 2, 1]), distance_sound)

resolve_td("TD 2.3.2.vingt_cinq", numpy.array(['UX', 'UX', 'V', 'X', 'X', 'V', 'UX', 'X', 'X', 'V']), numpy.concatenate((dtw_2_3_vingt, dtw_2_3_cinq)), numpy.array([1, 2, 1]), distance_sound)
resolve_td("TD 2.3.2.cent_cinq", numpy.array(['UX', 'UX', 'V', 'X', 'X', 'V', 'UX', 'X', 'X', 'V']), numpy.concatenate((dtw_2_3_cent, dtw_2_3_cinq)), numpy.array([1, 2, 1]), distance_sound)
resolve_td("TD 2.3.2.cent_vingt", numpy.array(['UX', 'UX', 'V', 'X', 'X', 'V', 'UX', 'X', 'X', 'V']), numpy.concatenate((dtw_2_3_cent, dtw_2_3_vingt)), numpy.array([1, 2, 1]), distance_sound)
resolve_td("TD 2.3.2.cent_vingt", numpy.array(['UX', 'UX', 'V', 'X', 'X', 'V', 'UX', 'X', 'X', 'V']), numpy.concatenate((dtw_2_3_cent, dtw_2_3_vingt)), numpy.array([1, 2, 1]), distance_sound, 0)
resolve_td("TD 2.3.2.cent_vingt", numpy.array(['UX', 'UX', 'V', 'X', 'X', 'V', 'UX', 'X', 'X', 'V']), numpy.concatenate((dtw_2_3_cent, dtw_2_3_vingt)), numpy.array([1, 2, 1]), distance_sound, 1)

#%% md

# Part II: Audio control word recognition system

On the shared space, you will find audio recordings of command words for a quadricopter drone composed of several male french speakers (noted M01...M13) and female french speakers (F01...F05).

You can thus divide all the data into learning bases that will serve as references and test bases to evaluate recognition by dynamic programming.

#%%

def distance_mfcc(_lhs, _rhs):
    _width = min(len(_lhs), len(_rhs))

    _output = 0

    for _i in range(0, _width):
            _output += distance_euclidean(_lhs[_i], _rhs[_i]) * distance_euclidean(_lhs[_i], _rhs[_i])

    return math.sqrt(_output)


#%% md

The following lines of code allow you to transform the audio file into a matrix of parameters called MFCC (Mel Frequency Cepstral Coefficient) using the *librosa* python library. These settings are used to extract the best possible frequency voice content from the audio signal.

The output matrix is composed of as many column vectors as analysis frames. The number of lines corresponds to the size of the representative vector: here 12.


#%% md

**Audio file upload:**

#%%

def load_norm(_path):
    _y, _sr = librosa.load(_path, sr=16000)
    _y_nr = nr.reduce_noise(audio_clip=_y, noise_clip=_y, verbose=False)
    _y_t, index = librosa.effects.trim(_y_nr, top_db=15)

    _scaler = sklearn.preprocessing.StandardScaler()
    _mfcc = librosa.feature.mfcc(y=_y_t, sr=_sr, hop_length=1024, htk=True, n_mfcc=12)
    _mfcc_process = _scaler.fit_transform(_mfcc)

    return _y, _y_nr, _sr, _mfcc_process

class Dataset:
    TEXTS = ["arretetoi", "atterrissage", "avance", "decollage", "droite", "etatdurgence", "faisunflip", "gauche", "plusbas", "plushaut", "recule", "tournedroite", "tournegauche"]

    def __init__(self, _path, _parser = lambda _name: [_name[:4], _name[4:-4], _name[-4:]]):
        self.names = []
        self.raws = []
        self.nrs = []
        self.srs = []
        self.mfccs = []

        _last = ""

        for _file in os.listdir(_path):
            _parsed_name = _parser(_file)
            if _parsed_name is not None:
                if _parsed_name[0] != _last:
                    self.names.append([None] * len(Dataset.TEXTS))
                    self.raws.append([None] * len(Dataset.TEXTS))
                    self.nrs.append([None] * len(Dataset.TEXTS))
                    self.srs.append([None] * len(Dataset.TEXTS))
                    self.mfccs.append([None] * len(Dataset.TEXTS))
                    _last = _parsed_name[0]
                _raw, _nr, _sr, _mfcc = load_norm(_path + "/" + _file)
                for _i in range(0, len(Dataset.TEXTS)):
                    if Dataset.TEXTS[_i] == _parsed_name[1]:
                        self.names[-1][_i] = _file
                        self.raws[-1][_i] = _raw
                        self.nrs[-1][_i] = _nr
                        self.srs[-1][_i] = _sr
                        self.mfccs[-1][_i] = _mfcc
                        break

without_noise_male = Dataset("../data/corpus/drone_withoutnoise", lambda name: [name[:4], name[4:-4], name[-4:]] if name[0] == "M" else None)
without_noise_female = Dataset("../data/corpus/drone_withoutnoise", lambda name: [name[:4], name[4:-4], name[-4:]] if name[0] == "F" else None)
# with_noise_male = Dataset("../data/corpus/drone_noise")
# custom = Dataset("../data/corpus/drone_custom")

#%% md

**MFCC extraction**

#%%

# influence of male voices vs female voices
# test with male - male, female - female, male - female, female - male

REF_MALE_NUM = 11
REF_FEMALE_NUM = 3

raw_ref_male = without_noise_male.raws[REF_MALE_NUM - 1]
nr_ref_male = without_noise_male.nrs[REF_MALE_NUM - 1]
sr_ref_male = without_noise_male.srs[REF_MALE_NUM - 1]
mfcc_ref_male = without_noise_male.mfccs[REF_MALE_NUM - 1]

raw_ref_female = without_noise_female.raws[REF_FEMALE_NUM - 1]
nr_ref_female = without_noise_female.nrs[REF_FEMALE_NUM - 1]
sr_ref_female = without_noise_female.srs[REF_FEMALE_NUM - 1]
mfcc_ref_female = without_noise_female.mfccs[REF_FEMALE_NUM - 1]

#%% md

**Application of DTW**

1. Carry out a study that you will detail on a report (for example, *influence male voices VS female voices, compare your own voice with the database, test the impact of different background noises on recognition...*) and create your own learning database and test database from the corpus and the voices and noises you have recorded.

2. Apply DTW to your corpora.

**Settings for audio recordings of your personal voices:**

16 KHz, mono, 16 bits, *.wav* format


#%%

mfcc_omegas = numpy.array([1, 1, 1])

without_noise_male_male_matrices = []
without_noise_male_male_paths = []
without_noise_male_male_ds = []

without_noise_male_female_matrices = []
without_noise_male_female_paths = []
without_noise_male_female_ds = []

without_noise_female_male_matrices = []
without_noise_female_male_paths = []
without_noise_female_male_ds = []

without_noise_female_female_matrices = []
without_noise_female_female_paths = []
without_noise_female_female_ds = []

for i in range(0, len(without_noise_male.names)):
    without_noise_male_male_matrices.append([])
    without_noise_male_male_paths.append([])
    without_noise_male_male_ds.append([])

    for j in range(0, len(Dataset.TEXTS)):
        without_noise_male_male_matrices[-1].append([None] * len(Dataset.TEXTS))
        without_noise_male_male_paths[-1].append([None] * len(Dataset.TEXTS))
        without_noise_male_male_ds[-1].append([None] * len(Dataset.TEXTS))

        if mfcc_ref_male[j] is not None:
            for k in range(0, len(Dataset.TEXTS)):
                 if without_noise_male.names[i][k] is not None:
                    without_noise_male_male_matrices[-1][-1][k] = dtw(mfcc_ref_male[j], without_noise_male.mfccs[i][k], mfcc_omegas, distance_mfcc)
                    without_noise_male_male_paths[-1][-1][k] = dtw_path(mfcc_ref_male[j], without_noise_male.mfccs[i][k], without_noise_male_male_matrices[-1][-1][k])
                    without_noise_male_male_ds[-1][-1][k] = dtw_d(mfcc_ref_male[j], without_noise_male.mfccs[i][k], without_noise_male_male_matrices[-1][-1][k])

for i in range(0, len(without_noise_female.names)):
    without_noise_male_female_matrices.append([])
    without_noise_male_female_paths.append([])
    without_noise_male_female_ds.append([])

    for j in range(0, len(Dataset.TEXTS)):
        without_noise_male_female_matrices[-1].append([None] * len(Dataset.TEXTS))
        without_noise_male_female_paths[-1].append([None] * len(Dataset.TEXTS))
        without_noise_male_female_ds[-1].append([None] * len(Dataset.TEXTS))

        if mfcc_ref_male[j] is not None:
            for k in range(0, len(Dataset.TEXTS)):
                 if without_noise_female.names[i][k] is not None:
                    without_noise_male_female_matrices[-1][-1][k] = dtw(mfcc_ref_male[j], without_noise_female.mfccs[i][k], mfcc_omegas, distance_mfcc)
                    without_noise_male_female_paths[-1][-1][k] = dtw_path(mfcc_ref_male[j], without_noise_female.mfccs[i][k], without_noise_male_female_matrices[-1][-1][k])
                    without_noise_male_female_ds[-1][-1][k] = dtw_d(mfcc_ref_male[j], without_noise_female.mfccs[i][k], without_noise_male_female_matrices[-1][-1][k])

for i in range(0, len(without_noise_male.names)):
    without_noise_female_male_matrices.append([])
    without_noise_female_male_paths.append([])
    without_noise_female_male_ds.append([])

    for j in range(0, len(Dataset.TEXTS)):
        without_noise_female_male_matrices[-1].append([None] * len(Dataset.TEXTS))
        without_noise_female_male_paths[-1].append([None] * len(Dataset.TEXTS))
        without_noise_female_male_ds[-1].append([None] * len(Dataset.TEXTS))

        if mfcc_ref_female[j] is not None:
            for k in range(0, len(Dataset.TEXTS)):
                 if without_noise_male.names[i][k] is not None:
                    without_noise_female_male_matrices[-1][-1][k] = dtw(mfcc_ref_female[j], without_noise_male.mfccs[i][k], mfcc_omegas, distance_mfcc)
                    without_noise_female_male_paths[-1][-1][k] = dtw_path(mfcc_ref_female[j], without_noise_male.mfccs[i][k], without_noise_female_male_matrices[-1][-1][k])
                    without_noise_female_male_ds[-1][-1][k] = dtw_d(mfcc_ref_female[j], without_noise_male.mfccs[i][k], without_noise_female_male_matrices[-1][-1][k])

for i in range(0, len(without_noise_female.names)):
    without_noise_female_female_matrices.append([])
    without_noise_female_female_paths.append([])
    without_noise_female_female_ds.append([])

    for j in range(0, len(Dataset.TEXTS)):
        without_noise_female_female_matrices[-1].append([None] * len(Dataset.TEXTS))
        without_noise_female_female_paths[-1].append([None] * len(Dataset.TEXTS))
        without_noise_female_female_ds[-1].append([None] * len(Dataset.TEXTS))

        if mfcc_ref_female[j] is not None:
            for k in range(0, len(Dataset.TEXTS)):
                 if without_noise_female.names[i][k] is not None:
                    without_noise_female_female_matrices[-1][-1][k] = dtw(mfcc_ref_female[j], without_noise_female.mfccs[i][k], mfcc_omegas, distance_mfcc)
                    without_noise_female_female_paths[-1][-1][k] = dtw_path(mfcc_ref_female[j], without_noise_female.mfccs[i][k], without_noise_female_female_matrices[-1][-1][k])
                    without_noise_female_female_ds[-1][-1][k] = dtw_d(mfcc_ref_female[j], without_noise_female.mfccs[i][k], without_noise_female_female_matrices[-1][-1][k])

#%% md

**Assessment of recognition**

1. Calculate the system confusion matrix (in line with the references and in column the system outputs).
You can use the *confusion_matrix* function of the *sklearn* library.


2. Calculate the recognition score: number of well recognized files on number of tested files.

*Verification:*
- if you use the M01 reference and test file, you must get no errors.
- if you use as M01 reference file and M02 test file, you must get two errors.

#%%

# skplot.metrics.plot_confusion_matrix()

print("male-male")
without_noise_male_male_total = 0
without_noise_male_male_valid = 0
for i in range(0, len(without_noise_male.names)):
    for j in range(0, len(Dataset.TEXTS)):
        without_noise_male_male_total += 1
        if j == without_noise_male_male_ds[i][j].index(min(without_noise_male_male_ds[i][j])):
            without_noise_male_male_valid += 1
print(without_noise_male_male_valid, " / ", without_noise_male_male_total)

print("male-female")
without_noise_male_female_total = 0
without_noise_male_female_valid = 0
for i in range(0, len(without_noise_female.names)):
    for j in range(0, len(Dataset.TEXTS)):
        without_noise_male_female_total += 1
        if j == without_noise_male_female_ds[i][j].index(min(without_noise_male_female_ds[i][j])):
            without_noise_male_female_valid += 1
print(without_noise_male_female_valid, " / ", without_noise_male_female_total)

print("female-male")
without_noise_female_male_total = 0
without_noise_female_male_valid = 0
for i in range(0, len(without_noise_male.names)):
    for j in range(0, len(Dataset.TEXTS)):
        without_noise_female_male_total += 1
        if j == without_noise_female_male_ds[i][j].index(min(without_noise_female_male_ds[i][j])):
            without_noise_female_male_valid += 1
print(without_noise_female_male_valid, " / ", without_noise_female_male_total)

print("female-female")
without_noise_female_female_total = 0
without_noise_female_female_valid = 0
for i in range(0, len(without_noise_female.names)):
    for j in range(0, len(Dataset.TEXTS)):
        without_noise_female_female_total += 1
        if j == without_noise_female_female_ds[i][j].index(min(without_noise_female_female_ds[i][j])):
            without_noise_female_female_valid += 1
print(without_noise_female_female_valid, " / ", without_noise_female_female_total)

#%% md

## Part III: Comparison of dynamic programming with a classification method after data pre-processing

In this section, we will compare the results of DTW with those of a data classification method: the k-nearest neighbors (k-nn).

We will use the functions to calculate the PCA and k-nn via the python library *scikit-learn*.


#%%

from sklearn.preprocessing import StandardScaler
from sklearn.neighbors import KNeighborsClassifier
from mpl_toolkits.mplot3d import Axes3D

#%% md

#### PCA preprocessing

To test a classification method, the size of the MFCCs must first be reduced:

1. From all the records in the learning database, perform a Principal Component Analysis (PCA) using the *PCA* function of the *scikit-learn* library and then project the test data into this new database.

*Note:* You can also implement the PCA by
extracting the 3 eigenvectors, noted $X_1$, $X_2$, $X_3$, associated with the 3 largest eigenvalues of the
variance-covariance $\Sigma_{App}$ (by the functions *np.cov* and *np.linalg.eig*). These eigenvectors will constitute the new benchmark P. Then project the data from the learning and test database into this new database by multiplying each vector by the database $P =[X_1X_2X_3]$.

#%%



#%% md

#### Classification by k nearest neighbors

In artificial intelligence, the k nearest neighbor method (*k-nn*) is a supervised method. In this context, there is a learning database of "label-data" pairs. To estimate the output associated with a new input $x$, the $k$ nearest neighbor  method consists of taking into account (in the same way) the $k$ learning samples whose input is closest to the new input $x$, according to a distance to be defined. The associated algorithm and an example are given below.

<img src="files/knn.jpg" width="700" height="500" >

<img src="files/kppv.png" width="300" height="300" >

**Example of classification by k-nn.** The test sample (green circle) must be classified either in the first
class of blue squares, or in the second class of red triangles.
If k = 3 (full circle), it is assigned to the second class because there are 2 triangles and only 1 square inside the inner circle.
If k = 5 (dotted circle), it is assigned to the first class (3 squares against 2 triangles inside the outer circle)


1. Using the *KNeighborsClassifier* function of the *sklearn.neighbors library*, perform a classification by k-nn on the learning and test basis you have predefined (take $k=1$).

2. Evaluate the k-nn method by calculating the confusion matrix and the recognition rate.

3. Change the value of $k$ for k-nn. Do you improve recognition scores?

4. Compare your results with those of DTW.

#%%


